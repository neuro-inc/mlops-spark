{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b4a257",
   "metadata": {},
   "source": [
    "### Readme\n",
    "\n",
    "Welcome to Jupyter Notebooks on [Neu.ro](https://neu.ro).\n",
    "\n",
    "This guide contains simplistic example of running Jupyter instance with PySpark application driver.\n",
    "\n",
    "It assumes you have got a kubectl [config file](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/) and mounted it into the job at `/home/jovyan/.kube/config`. If the mount point is not a default, set `KUBECONFIG` environment variable ponting to the file location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68159e10-55e0-411d-8d02-7b4e36eb1bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "from random import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc8bd7-7e54-4db8-96f9-38324c23ca54",
   "metadata": {},
   "source": [
    "#### Example context configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bfe28c-5cce-4c02-84a4-f278be6352e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T14:56:55.219751Z",
     "iopub.status.busy": "2023-10-30T14:56:55.219291Z",
     "iopub.status.idle": "2023-10-30T14:57:01.714898Z",
     "shell.execute_reply": "2023-10-30T14:57:01.714392Z",
     "shell.execute_reply.started": "2023-10-30T14:56:55.219726Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_app_namespace = \"\"  # use namespace provided by Neu.ro team\n",
    "driver_service_account = \"\"  # use service account name provided by Neu.ro team\n",
    "k8s_api_address = \"\" # kubernetes API endpoint provided by Neu.ro team, e.g. https://x.x.x.x:6443\n",
    "\n",
    "config = {\n",
    "    \n",
    "    \"spark.kubernetes.namespace\": spark_app_namespace,\n",
    "    # If changed, should contain the same version of PySpark, Hadoop and Hadoop-AWS libs\n",
    "    \"spark.kubernetes.container.image\": \"ghcr.io/neuro-inc/pyspark:pipelines\",\n",
    "    \"spark.kubernetes.authenticate.driver.serviceAccountName\": driver_service_account,\n",
    "    \"spark.kubernetes.authenticate.serviceAccountName\": driver_service_account,\n",
    "    # This might be handy for debugging\n",
    "    #\"spark.kubernetes.executor.deleteOnTermination\": \"false\",\n",
    "    \"spark.executor.instances\": \"2\",\n",
    "    \"spark.executor.memory\": \"1g\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.driver.blockManager.port\": \"7777\",\n",
    "    \"spark.driver.port\": \"2222\",\n",
    "    # Driver job internal hostname\n",
    "    \"spark.driver.host\": os.environ.get(\"NEURO_JOB_INTERNAL_HOSTNAME\"),\n",
    "    \"spark.driver.bindAddress\": \"0.0.0.0\",\n",
    "    # If neuro blob access is needed, uncomment and adjust the following configurations:\n",
    "    # \"spark.hadoop.fs.s3a.endpoint\": endpoint_url from `neuro blob mkcredentials`,\n",
    "    # \"spark.hadoop.fs.s3a.connection.ssl.enabled\": \"true\",\n",
    "    # \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n",
    "    # \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    # \"spark.hadoop.com.amazonaws.services.s3.enableV4\": \"true\",\n",
    "    # \"spark.hadoop.fs.s3a.access.key\": access_key_id from `neuro blob mkcredentials`,\n",
    "    # \"spark.hadoop.fs.fs.s3a.secret.key\": secret_access_key from `neuro blob mkcredentials`,\n",
    "    # \"spark.hadoop.fs.s3a.aws.credentials.provider\": \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",\n",
    "}\n",
    "\n",
    "def get_spark_session(app_name: str, conf: SparkConf):\n",
    "    conf.setMaster(f\"k8s://{k8s_api_address}\")\n",
    "    for key, value in config.items():\n",
    "        conf.set(key, value)    \n",
    "    return SparkSession.builder.appName(app_name).config(conf=conf).getOrCreate()\n",
    "spark = get_spark_session(\"MyTestApp\", conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4496fe20-83e4-4047-9315-b615cf8a1d3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-30T14:57:01.716014Z",
     "iopub.status.busy": "2023-10-30T14:57:01.715847Z",
     "iopub.status.idle": "2023-10-30T14:57:02.852761Z",
     "shell.execute_reply": "2023-10-30T14:57:02.851717Z",
     "shell.execute_reply.started": "2023-10-30T14:57:01.715995Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "partitions = 2\n",
    "n = 100000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926c584-5b2d-4778-97a8-942766f18467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
